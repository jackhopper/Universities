---
title: "University Clustering"
author: "Jack Hopper"
date: "12/4/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction - Why Do This Analysis?

One of the most interesting Unsupervised Machine Learning techniques is clustering. Clustering helps us find interesting ways to group items based on how similar they are. One useful application of clustering is in higher education. For example, consider two schools in my home state of Ohio. Should we think of the College of Wooster as a comparable school to the University of Dayton? Both are private colleges that enroll less than 10,000 students, have a healthy share of out-of-state students, and charge relatively high (>$40,000) tuition. When we're classifying universities, should they be in the same group?

For this analysis, I'll attempt to classify schools based on their selectivity. 'Selectivity' is a vague metric, but there are a handful of meaningful statistics that represent selectivity based on university prestige and academic rigor. These are metrics such as tuition rates, admission rates, ACT scores for incoming students, and total enrollment.

### Data Gathering & Exploration

For this analysis, we will make use of the IPEDS (Integrated Postsecondary Education Data System) database, a government source for higher education data. The latest data covers the 2019-2020 school year.

IPEDS publishes a Microsoft Access database every year with hundreds of variables covering thousands of higher education institutions. To simplify things, I exported the relevant tables from Access to Excel, to streamline the analysis & move away from the bulky Access database.

#### Load packages and connect to the Access database

For this analysis, we need the tidyverse as well as the readxl package to load the Excel data.

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(readxl)
```

```{r, include=FALSE}
setwd("C:\\Users\\jackh\\OneDrive\\Documents\\R Analytics\\Universities")
```

```{r}
# Read data
institution <- read_excel("data\\HD2021.xlsx")
adm <- read_excel("data\\ADM2021.xlsx")
adm_yield <- read_excel("data\\DRVADM2021.xlsx")
enrollment <- read_excel("data\\DRVEF2021.xlsx")
enrollment_d <- read_excel("data\\EF2021D.xlsx")
tuition <- read_excel("data\\IC2021_AY.xlsx")
```

##### Build the data frame

We will require data on multiple categories. Note that IPEDS stores each variable with a mnemonic - so note that I have taken steps to translate each mnemonic into a more informative variable name.

```{r}
#Institutional characteristics

institution <- institution %>% 
  select(UNITID, INSTNM, ICLEVEL, CONTROL,  LATITUDE, LONGITUD, ADDR, CITY, STABBR, ZIP, CBSA) %>% 
  rename(school = INSTNM)

#Admissions
adm <- adm %>% 
  select(UNITID, ACTCM75) %>% 
  rename(act_score = ACTCM75
         )
#DRAdmissions table
adm_yield <- adm_yield %>% 
  select(UNITID, DVADM01) %>% 
  rename(pct_admitted = DVADM01)

#Enrollment characteristics
enrollment <- enrollment %>% 
  select(UNITID, EFUG, EFUGFT, EFUGPT, PCUDEEXC, RMINSTTP, RMOUSTTP, RMFRGNCP) %>% 
  rename(ug_enroll = EFUG,
         ft_enroll = EFUGFT,
         pt_enroll = EFUGPT,
         pct_online = PCUDEEXC,
         pct_instate = RMINSTTP,
         pct_outstate = RMOUSTTP,
         pct_foreign = RMFRGNCP)
enrollment_d <- enrollment_d %>%
  select(UNITID, RET_PCF) %>% 
  rename(retention_rate = RET_PCF)
enrollment <- left_join(enrollment, enrollment_d, by = c("UNITID" = "UNITID"))

#Tuition
tuition <- tuition %>% 
  select(UNITID, TUITION2, TUITION3) %>% 
  rename(in_state_tuition = TUITION2,
         out_state_tuition = TUITION3)
```

Now we can join all this data together into a main data frame and filter the resulting data to only include 4-year, non-profit schools (ie, exclude 2 year/technical colleges and for-profit institutions, as those are outside the scope of this analysis).
```{r}
###Join it all together###
school_data <- institution %>% 
  left_join(y = adm, by = c("UNITID" = "UNITID")) %>% 
  left_join(y = adm_yield, by = c("UNITID" = "UNITID")) %>% 
  left_join(y = enrollment, by = c("UNITID" = "UNITID")) %>% 
  left_join(y = tuition, by = c("UNITID" = "UNITID")) 
#And limit the analysis to 4 year private & public non-profit institutions
school_data <- school_data %>% 
  filter(ICLEVEL == 1,
         CONTROL != 3) %>% 
  select(-ICLEVEL)

#Let's create a key linking each school ID to a school, for later
school_id_key <- school_data %>% 
  select(UNITID, school)

```

#### Data Cleaning

We now have a primary data frame. Let's take a look at it.
```{r}
school_data <- school_data %>% 
  mutate(UNITID = as.character(UNITID))

summary(school_data)
```

Now we can engage in some light feature engineering. One of these features will be to calculate an 'average' tuition rate for all students.

```{r}
school_data <- school_data %>% 
  mutate(in_state_share = pct_instate/ (pct_instate + pct_outstate),
         out_state_share = 1-in_state_share,
         avg_tuition = (in_state_share * in_state_tuition) + (out_state_share * out_state_tuition)
         ) %>% 
  select(-in_state_share, -out_state_share)

summary(school_data$avg_tuition)
```

One thing I noticed about the data in the first exploration is that the ACT score and percent admitted fields seem to have the most NAs. We can use of the plot_histogram() feature from the DataExplorer package to give a better sense of the NA data here.

```{r, echo=FALSE}
library(DataExplorer)

#Could view a table of missing data here...
#school_data %>% 
#  filter(is.na(act_score)) %>% 
#  summary()

#...or get a visual view of missing data here.
school_data %>% 
  filter(is.na(act_score)) %>% 
  plot_histogram()
```
There doesn't seem to be a unifying feature that links each school with missing ACT scores. Most schools are smaller, but there are some larger schools being left out. Accordingly, we can impute this missing data with the average value in our data, so we aren't losing all these observations.

```{r}
school_data <- school_data %>% 
  mutate(act_score = ifelse(is.na(act_score), mean(act_score, na.rm = TRUE), act_score))
```

Finally, k-means can only handle numeric data so we will remove any categorical features.

```{r}
school_full <- school_data

school_data <- school_data %>% 
  filter(!is.na(act_score)) %>% 
  drop_na()

modeled_data <- school_data %>% 
  column_to_rownames(var = 'UNITID') %>% 
  mutate(pct_admitted = ifelse(is.na(pct_admitted), imputed_admissions, pct_admitted)) %>% 
  select(-ft_enroll, -pt_enroll, -pct_online, -pct_foreign, -retention_rate, -in_state_tuition, -out_state_tuition, -school,
         -CITY, -STABBR, -ZIP, -CBSA, -LATITUDE, -LONGITUD, -ADDR, -CONTROL) #These variables can explain the results of the analysis but themselves don't define 'selectivity' in our analysis

plot_histogram(modeled_data)
summary(modeled_data)
```

Now that we cleaned up the data, we can look at what fields will go into our k-means analysis.
```{r}
colnames(modeled_data)
```

### K-Means Analysis

We have a nice, tidy dataset.

Now we can start analyzing our data! K-means requires that all data is on the same magnitude, so that fields with large values or wide arrays of values don't have an unfair influence on the dataset. We can do that easily with the scale() function.

```{r cleaning}
modeled_data <- modeled_data %>% 
  scale()

summary(modeled_data)
```
#### Choosing the right value for 'k'
While the 'optimal' number of clusters is ultimately a judgment call, there are a few ways to determine how many clusters we should settle with. There are three quantitative measures that help us here:
1) the 'elbow' method, which calculates the sum of distances between each item within a cluster for various values of k -- and the 'elbow point' is the optimal number of clusters, since this is the point at which increasing k no longer yields a significant improvement in the total sum of squares;
2) The Silhouette Method - which calculates how closely an item is matched with other items in the same cluster and how loosely it is with other clusters, where a high average value is optimal; and
3) The Gap Statistic, which compares the difference between clusters in an observed dataset and clusters in randomly-generated datasets. 
These three methods are ultimately suggestions, and it's up to the analyst to choose the optimal k.

```{r, message=FALSE}
library(factoextra)
###Choose the right k
#Elbow method
fviz_nbclust(modeled_data, kmeans, method = "wss") #This suggests that 2 or 4 is the optimal k
#Average silhouette method
fviz_nbclust(modeled_data, kmeans, method = "silhouette") #This suggests optimal k is 2
fviz_nbclust(modeled_data, kmeans, method = "gap_stat") #This suggests 10 is the optimal k

```

It seems like two of the three methods seem to point to 2 clusters.  While there is no 'one' answer to this question, intuitively it makes sense to have somewhere between 3-6 groups for easy interpretation.

#### Performing clustering
```{r}
set.seed(1234)
k_2 <- kmeans(modeled_data, centers = 2, nstart = 50)
k_4 <- kmeans(modeled_data, centers = 4, nstart = 50)

k_2$size
k_4$size

#Assign each school to a cluster
school_data <- school_data %>% 
  mutate(cluster_2 = k_2$cluster) %>% 
  mutate(cluster_4 = k_4$cluster)
```



#### Analyzing results
We can start by simply summarizing each of the resulting groups across our frameworks.
```{r}
school_data %>% 
  select(cluster_2, CONTROL, act_score, pct_admitted, ug_enroll, pct_online, pct_instate, avg_tuition) %>% 
  group_by(cluster_2) %>% 
  summarize_all('mean')

school_data %>% 
  select(cluster_4, CONTROL, act_score, pct_admitted, ug_enroll, pct_online, pct_instate, avg_tuition) %>% 
  group_by(cluster_4) %>% 
  summarize_all('mean')
```

We can also visualize the results. Note that this chart reduces the data into a two-dimensional graph via Principal Component Analysis, which makes the chart much less interpretable... but nonetheless gives a good sense of the rough shapes and overlaps present within our clusters.
```{r warning=FALSE, message=FALSE}
fviz_cluster(k_2,
             data = modeled_data,
             repel = TRUE,
             geom = "point",
             ggtheme = theme_minimal(),
             title = "Clustering Results - 2 Clusters")

fviz_cluster(k_4,
             data = modeled_data,
             repel = TRUE,
             geom = "point",
             ggtheme = theme_minimal(),
             title = "Clustering Results - 4 Clusters")
```

To me, two groups doesn't seem descriptive enough, so sticking with 4 groups feels right.

Let's see the summary table on the four-group split again.
```{r, message=FALSE, warning=FALSE}
library(kableExtra)
school_data %>% 
  select(cluster_4, CONTROL, act_score, pct_admitted, ug_enroll, pct_online, pct_instate, avg_tuition) %>% 
  group_by(cluster_4) %>% 
  summarize_all('mean') %>%
  kable(digits=2, format = 'markdown',row.names = TRUE) %>% 
  kable_styling(full_width = T,
                font_size = 12) %>% 
  row_spec(row = 0, color = '#660033')
```

### Evaluating the Clusters

- Group 1 are the 'elites'. This group stands out from the rest on its ability to charge a lot in average tuition -- over \$50,000 on average! -- and accepts the lowest percentage of students.
- Group 2 is the less selective category. They have the lowest entrance test requirements and the highest share of students that come from within their state. Although not every school in this group is public, schools in this group also charge a low average tuition.
- Group 3 is the 'small privates'. Although this group has, in general, similar academic requirements to Group 2 & 4, they charge over twice as much. These schools are small, on average, and they do a better job at attracting students from out-of-state.
- Group 4 is the 'large publics' bucket - nearly every school in this group is a public school. The average tuition in this group is quite low, with an average of <$13,000. Based on admissions standards, this group is moderately selective.

It's interesting that even though control is not a metric included in the visualization, the four groups have a relatively clean split between public and private ones.

Now that we have a sense of what makes up our data, we can give our clusters some more informative names.

```{r, warning=FALSE}

summary_table <- school_data %>% 
    select(cluster_4, CONTROL, act_score, pct_admitted, ug_enroll, pct_online, pct_instate, avg_tuition, retention_rate) %>% 
  group_by(cluster_4) %>% 
  summarize_all('mean')

summary_table <- summary_table %>% 
  mutate(cluster_4 = ifelse(cluster_4=='1', 'Large Publics',
                            ifelse(cluster_4=='2', 'Less Selective',
                                   ifelse(cluster_4=='3', 'Small Privates', 'Elites')))) %>% 
  rename(cluster = cluster_4,
         control = CONTROL)

kable(summary_table, 'html', digits = c(0,2,2,2,0,1,1,0,2), align = 'lcccccccc') %>% 
  kable_styling(full_width = F)
```

Let's drive this home with some visuals.
```{r}
school_data <- school_data %>% 
  select(-cluster_2) %>% 
  rename(cluster= cluster_4) %>% 
  mutate(cluster = ifelse(cluster=='1', 'Elites',
                            ifelse(cluster=='2', 'Less Selective',
                                   ifelse(cluster=='3', 'Small Privates', 'Large Publics')))) %>% 
  mutate(cluster = as.factor(cluster))
```

```{r}
library(RColorBrewer) #for pretty colors
#Average selectivity metrics by cluster
school_data %>% 
  select(cluster, act_score, pct_admitted, avg_tuition, ug_enroll) %>% 
  group_by(cluster) %>% 
  summarize_all('mean') %>% 
  pivot_longer(cols = -cluster) %>% 
  ggplot() +
  geom_col(aes(x = cluster, y = value, fill = cluster)) +
  facet_wrap(~name, scales = 'free') + 
  scale_fill_brewer(palette = 'Dark2') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

#Scatterplot - tuition and act scores
# my_plot <- school_data %>% 
#   rename(undergrad_enrollment = ug_enroll) %>% 
#   ggplot() +
#   geom_point(aes(x = act_score, y = avg_tuition, color = cluster, size = undergrad_enrollment), alpha = 0.5) +
#   scale_color_brewer(palette = 'Dark2') +
#   theme_minimal() +
#   labs(x = 'ACT Score (Composite, 75th Percentile)',
#        y = 'Average Tuition Rate',
#        title = 'University Selectivity - ACT Scores and Avg. Tuition by Cluster')

#ggplotly(my_plot)  # Convert to Plotly object

plot_ly(data = school_data, 
        x = ~act_score, y = ~avg_tuition, type = "scatter", mode = "markers",
        color = ~cluster,
        size = ~ug_enroll,
        hoverinfo = "text", text = ~paste("School: ", school, "<br> Undergrad Enrollment: ", ug_enroll)) %>%  # Your existing plot code
  layout(title = "Selectivity Metrics for Schools Based on Cluster",
         xaxis = list(title = "ACT Score (Composite, 75th Percentile)"),
         yaxis = list(title = "Average Tuition Rate"))


```


### Wrap-Up
So there you have it. Even though individual schools can have similar values for various selectivity measures, k-means helps us tell whether a school really is similar to another, not just based on one or two metrics, but the sum total of all the metrics that we deem important.

A good extension of this analysis could be to use the results of these groupings in a predictive model, allowing you to build different models for each cluster of schools. This is potentially useful since each resulting model's coefficients need only to describe a certain subset of the data as opposed to all the data, which could ultimately result in a more accurate model. 

```{r include=FALSE}
library(tidytext) #for the reorder_within function only
oh <- school_data %>% 
  filter(STABBR=='OH')
samples <- c('Youngstown State University', 'Case Western Reserve University', 'Ohio State University-Main Campus', 'Marietta College')

oh %>% 
  select(school, act_score, pct_admitted, avg_tuition, ug_enroll, cluster) %>% 
  rename(Cluster=cluster, `Percent Admitted`=pct_admitted, `Average Tuition`=avg_tuition, `Undergrad Enrollment` = ug_enroll) %>% 
  filter(!is.na(school)) %>% 
  pivot_longer(cols = c(-school, -Cluster)) %>% 
  arrange(Cluster, desc(value)) %>% 
  mutate(school = reorder_within(school, value, name)) %>% 
  ggplot() +
  geom_col(aes(x = reorder(school, -value), y = value, fill = Cluster), color='black', size=0.01) +
  facet_wrap(~name, scales = 'free') + 
  scale_fill_brewer(palette = 'Set1') +
  labs(x='Metric', y='Value', title='Selectivity Metrics for Ohio Schools Based on Cluster') +
  theme_minimal() +
  theme(axis.text.x = element_blank())

test <- oh %>% 
  filter(school %in% c('The College of Wooster', 'Xavier University'))
```

And what about those schools I brought up in the beginning, College of Wooster and Dayton? Turns out they do belong to different groups in our analysis, with Dayton in the 'Small Privates' group and Wooster in the 'Elites' group.
```{r}
library(ggrepel)
school_data %>% 
  rename(undergrad_enrollment = ug_enroll) %>% 
  ggplot(aes(x = act_score, y = avg_tuition)) +
  geom_point(aes(color = cluster, size = undergrad_enrollment), alpha = 0.5) +
  geom_text_repel(aes(label=ifelse(school == 'The College of Wooster' | school == 'University of Dayton', school, '')), nudge_x = 2, nudge_y = -4500, max.overlaps = 2000) +
  scale_color_brewer(palette = 'Dark2') +
  theme_minimal() +
  labs(x = 'ACT Score (Composite, 75th Percentile)',
       y = 'Average Tuition Rate',
       title = 'University Selectivity - ACT Scores and Avg. Tuition by Cluster')
```
